{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HW3P2",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyPzjWQ644n25MwPhQ9P2ggV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/manashpratim/Speech-To-Phenome/blob/master/HW3P2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a1lmzpq_xCKA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "swMlXlWwxDeZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn.utils.rnn import *\n",
        "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "DEVICE"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8jgXD-OAxNDI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!git clone --recursive https://github.com/parlance/ctcdecode.git\n",
        "!pip install wget\n",
        "%cd ctcdecode\n",
        "!pip install .\n",
        "%cd .."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hoZ6rSk1xupl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%cd /content/drive/My Drive/11-785-s20-hw3p2/hw3p2/\n",
        "from phoneme_list import PHONEME_MAP as phonemes\n",
        "from phoneme_list import PHONEME_LIST as phonemes_list\n",
        "%cd /content/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AvpHJrnPxzii",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install python-Levenshtein\n",
        "import Levenshtein"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "--8lDFFwlM0P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train = np.load('/content/drive/My Drive/11-785-s20-hw3p2/hw3p2/wsj0_train',allow_pickle=True,encoding = 'latin1')\n",
        "train_labels = np.load('/content/drive/My Drive/11-785-s20-hw3p2/hw3p2/wsj0_train_merged_labels.npy',allow_pickle=True,encoding = 'latin1')\n",
        "dev = np.load('/content/drive/My Drive/11-785-s20-hw3p2/hw3p2/wsj0_dev.npy',allow_pickle=True,encoding = 'latin1')\n",
        "dev_labels =  np.load('/content/drive/My Drive/11-785-s20-hw3p2/hw3p2/wsj0_dev_merged_labels.npy',allow_pickle=True,encoding = 'latin1')\n",
        "test = np.load('/content/drive/My Drive/11-785-s20-hw3p2/hw3p2/wsj0_test',allow_pickle=True,encoding = 'latin1')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dyI8117UdgxL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def preprocessing(arr,flag=False):\n",
        "    mean =  np.mean(arr,axis=0)\n",
        "    std = np.std(arr,axis=0)\n",
        "    if flag:\n",
        "      arr = (arr-mean)/std\n",
        "    else:\n",
        "      arr = (arr-mean)\n",
        "    return arr"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5gdJO_bvyACu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MyDataset(Dataset):\n",
        "  \n",
        "    def __init__(self,x,y):\n",
        "         self.X = [torch.FloatTensor(preprocessing(word,flag=True)) for word in x]\n",
        "         self.Y = [torch.LongTensor(word) for word in y]\n",
        "    \n",
        "    def __getitem__(self,i):\n",
        "\n",
        "        return self.X[i],self.Y[i]\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.X )\n",
        "\n",
        "def collate(seq_list):\n",
        "    inputs = [i[0] for i in seq_list]\n",
        "    targets = [i[1] for i in seq_list]\n",
        "    inputs = pad_sequence(inputs)\n",
        "    targets = pad_sequence(targets,batch_first=True, padding_value = 100000)\n",
        "    X_lens = torch.LongTensor([len(seq[0]) for seq in seq_list])\n",
        "    Y_lens = torch.LongTensor([len(seq[1]) for seq in seq_list])\n",
        "    return inputs,targets,X_lens,Y_lens"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YmQWZx_TyGB0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class TestDataset(Dataset):\n",
        "    \n",
        "    def __init__(self,x):\n",
        "         self.X = [torch.FloatTensor(preprocessing(word,flag=True)) for word in x]\n",
        "    \n",
        "    def __getitem__(self,i):\n",
        "\n",
        "        return self.X[i]\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.X )\n",
        "\n",
        "def collate_test(seq_list):\n",
        "    inputs = [i for i in seq_list]\n",
        "    inputs = pad_sequence(inputs)\n",
        "    X_lens = torch.LongTensor([len(seq) for seq in seq_list])\n",
        "    return inputs,X_lens"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ynmZKF03yWfE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_dataset = MyDataset(train,train_labels)\n",
        "val_dataset = MyDataset(dev,dev_labels)\n",
        "train_loader = DataLoader(train_dataset, shuffle=True, batch_size=64, collate_fn = collate,num_workers=8, pin_memory=True)\n",
        "val_loader = DataLoader(val_dataset, shuffle=False, batch_size=64, collate_fn = collate,num_workers=8, pin_memory=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NSSt2n-j1hgZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_dataset = TestDataset(test)\n",
        "test_loader = DataLoader(test_dataset, shuffle=False, batch_size=64, collate_fn = collate_test,num_workers=8, pin_memory=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x0ElpkXLyf4Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Model(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Model, self).__init__()\n",
        "        self.cnn1 = torch.nn.Conv1d(40, 256, 1, stride=1, padding=0,bias=False)\n",
        "        self.cnn2 = torch.nn.Conv1d(256, 256, 1, stride=1, padding=0,bias=False)\n",
        "        self.tanh = nn.Hardtanh()\n",
        "        self.lstm1 = nn.LSTM(256, 512, bidirectional=True,num_layers=6,dropout=0.2)\n",
        "        self.output = nn.Linear(512*2, 47)\n",
        "    \n",
        "    def forward(self, X, lengths):\n",
        "\n",
        "        X = self.tanh(self.cnn1(X.transpose(1,2)))\n",
        "        X = self.tanh(self.cnn2(X))\n",
        "        X = X.transpose(1,2)\n",
        "        packed_X = pack_padded_sequence(X, lengths, enforce_sorted=False)\n",
        "        packed_out = self.lstm1(packed_X)[0]\n",
        "        out, out_lens = pad_packed_sequence(packed_out)\n",
        "        out = self.output(out)\n",
        "        return out, out_lens"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b0YOc3ZxU7tQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def init_weights(m):\n",
        "    if type(m) == nn.Conv1d or type(m) == nn.Linear:\n",
        "        torch.nn.init.xavier_uniform_(m.weight.data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S3eFGsy0y82X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = Model()\n",
        "criterion = nn.CTCLoss(blank = 46)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "model.apply(init_weights)\n",
        "model = model.to(DEVICE)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BmtuFKSuzGa3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from ctcdecode import CTCBeamDecoder\n",
        "decoder = CTCBeamDecoder(['$']*47, beam_width=100, log_probs_input=True,blank_id = 46)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IXZb4bBszHTo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "criterion = criterion.to(DEVICE)\n",
        "for epoch in range(10):\n",
        "  los = 0 \n",
        "  total = 0\n",
        "  model.train()\n",
        "  for batch_idx, (inputs,targets,xlens,ylens) in enumerate(train_loader):\n",
        "      optimizer.zero_grad()\n",
        "      inputs = inputs.to(DEVICE)\n",
        "      targets = targets.to(DEVICE)\n",
        "      ylens = ylens.to(DEVICE)\n",
        "      xlens = xlens.to(DEVICE)\n",
        "      out, out_lens = model(inputs, xlens)\n",
        "      out = out.log_softmax(2)\n",
        "      loss = criterion(out, targets, out_lens, ylens)\n",
        "      los = los + loss.item()\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      del inputs,loss,out,targets,out_lens,xlens,ylens\n",
        "\n",
        "\n",
        "  print('Epoch', epoch + 1, 'Loss', los/ len(train_loader))\n",
        "\n",
        "  with torch.no_grad():\n",
        "      model.eval()\n",
        "      dis = 0\n",
        "      tot = 0 \n",
        "      los1 = 0\n",
        "      for batch_idx, (inputs,targets,xlens,ylens) in enumerate(val_loader):\n",
        "        inputs = inputs.to(DEVICE)\n",
        "        xlens = xlens.to(DEVICE)\n",
        "        out, out_lens = model(inputs, xlens)\n",
        "        out = out.log_softmax(2)\n",
        "        loss = criterion(out, targets.to(DEVICE), out_lens, ylens.to(DEVICE))\n",
        "        los1 = los1 + loss.item()\n",
        "        out = out.cpu()\n",
        "        out_lens = out_lens.cpu()\n",
        "        test_Y, _, _, test_Y_lens = decoder.decode(out.transpose(0, 1), out_lens)\n",
        "        \n",
        "        for i in range(out.size()[1]):\n",
        "            best_seq = test_Y[i, 0, :test_Y_lens[i, 0]]\n",
        "            best_pron = ''.join(phonemes[j] for j in best_seq)\n",
        "            best_tar = ''.join(phonemes[k] for k in targets[i] if k!=100000)\n",
        "            dis = dis +  Levenshtein.distance(best_pron, best_tar)\n",
        "            tot = tot + 1\n",
        "        del inputs,targets,out_lens,del out,xlens\n",
        "      print('Distance: ',dis/tot)\n",
        "      print('Loss (Val): ',los1/len(val_loader))\n",
        "     \n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bpWENDtpM6Zj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "torch.save(model.state_dict(), '/content/drive/My Drive/modelhw32.pt')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wAE0FvI_zZ0N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "output = []\n",
        "with torch.no_grad():\n",
        "      model.eval()\n",
        "      for batch_idx, (inputs,xlens) in enumerate(test_loader):\n",
        "        inputs = inputs.float().to(DEVICE)\n",
        "        \n",
        "        xlens = xlens.to(DEVICE)\n",
        "        out, out_lens = model(inputs, xlens)\n",
        "        out = out.cpu()\n",
        "        out_lens = out_lens.cpu()\n",
        "        test_Y, _, _, test_Y_lens = decoder.decode(out.transpose(0, 1), out_lens)\n",
        "        \n",
        "        for i in range(out.size()[1]):\n",
        "            best_seq = test_Y[i, 0, :test_Y_lens[i, 0]]\n",
        "            best_pron = ''.join(phonemes[j] for j in best_seq)\n",
        "            output.append(best_pron)\n",
        "        del inputs,out_lens,out,xlens"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q2LjdX4Gzk6T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open('/content/drive/My Drive/mbarman.csv', 'w') as w:\n",
        "    w.write('Id,Predicted\\n')\n",
        "    for i in range(len(output)):\n",
        "            w.write(str(i)+','+str(output[i])+'\\n')"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}